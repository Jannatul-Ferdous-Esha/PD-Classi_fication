{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXXf16Y73jmp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "# SelfAttention Layer\n",
        "class SelfAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.Wq = Dense(units, use_bias=False)\n",
        "        self.Wk = Dense(units, use_bias=False)\n",
        "        self.Wv = Dense(units, use_bias=False)\n",
        "\n",
        "    def call(self, x):\n",
        "        q = self.Wq(x)\n",
        "        k = self.Wk(x)\n",
        "        v = self.Wv(x)\n",
        "        attention_scores = tf.matmul(q, k, transpose_b=True)\n",
        "        attention_weights = tf.nn.softmax(attention_scores)\n",
        "        attention_output = tf.matmul(attention_weights, v)\n",
        "        return attention_output + x\n",
        "\n",
        "def build_optimized_model():\n",
        "    input_shape = (10, 256, 256, 1)\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Optimal hyperparameters\n",
        "    filters = 64\n",
        "    kernel_size = 5\n",
        "    lstm_units = 32\n",
        "    dropout_rate = 0.3\n",
        "    dense_units = 64\n",
        "    learning_rate = 0.00020063\n",
        "\n",
        "    # Convolution and Pooling Layers\n",
        "    x = TimeDistributed(Conv2D(filters, (kernel_size, kernel_size), activation='relu'))(inputs)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "    x = TimeDistributed(Conv2D(filters * 2, (kernel_size, kernel_size), activation='relu'))(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # LSTM Layer\n",
        "    x = LSTM(lstm_units, return_sequences=True)(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Self-Attention Layer\n",
        "    x = SelfAttention(lstm_units)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Dense Layer\n",
        "    x = Dense(dense_units, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Output Layer\n",
        "    outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "# Assuming 'train_dataset', 'val_dataset', and 'test_dataset' are defined\n",
        "model = build_optimized_model()\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "results = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {results[0]}\")\n",
        "print(f\"Test Accuracy: {results[1]}\")\n",
        "print(f\"Test Precision: {results[2]}\")\n",
        "print(f\"Test Recall: {results[3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bcQ81Hnr3kZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}